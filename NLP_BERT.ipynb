{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhsn0b6P5D7iM+0mMDpZ1H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhien-tan/AI-Assignment/blob/main/NLP_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QplbO_vXMSHe"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install transformers datasets -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import wandb\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "CzDe2U4cXJ4q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load a sample sentiment dataset (IMDB)\n",
        "# Load CSV\n",
        "wandb.login(key='0d5cd9edd004ca35504ccfdaea311fd22631abc5')\n",
        "df = pd.read_csv('/content/coffee.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Create a sentiment label: 1-2 stars = negative (0), 3 = neutral (1), 4-5 = positive (2)\n",
        "def map_sentiment(stars):\n",
        "    if stars <= 2:\n",
        "        return 0  # Negative\n",
        "    elif stars == 3:\n",
        "        return 1  # Neutral\n",
        "    else:\n",
        "        return 2  # Positive\n",
        "\n",
        "df['label'] = df['stars'].apply(map_sentiment)\n",
        "df = df[['reviews', 'label']]\n",
        "\n"
      ],
      "metadata": {
        "id": "1cWV3jEEXR49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Step 4: Split the Dataset\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(df['reviews'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict({'text': train_texts.tolist(), 'label': train_labels.tolist()})\n",
        "test_dataset = Dataset.from_dict({'text': test_texts.tolist(), 'label': test_labels.tolist()})\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IINGOgLf-RwH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n"
      ],
      "metadata": {
        "id": "28zwE1xZXWmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n"
      ],
      "metadata": {
        "id": "fL7oqsnIXYLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(axis=1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n"
      ],
      "metadata": {
        "id": "adZgKrLcXa2g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fzpysZ8WXdCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "kjdIy-xH5sWn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fb0rTB-fBESF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(f\"Accuracy: {results['eval_accuracy']:.2f}\")\n"
      ],
      "metadata": {
        "id": "KA7Kbe8m8h-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrix"
      ],
      "metadata": {
        "id": "nGhm0rkHRMRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions_output = trainer.predict(train_dataset)\n",
        "\n",
        "# Get predicted labels\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
        "\n",
        "# True labels from the dataset\n",
        "y_true = predictions_output.label_ids\n",
        "\n",
        "# Print classification report\n",
        "print(\"ðŸ“Š Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nðŸ§® Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Optionally, plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Neg\", \"Neu\", \"Pos\"], yticklabels=[\"Neg\", \"Neu\", \"Pos\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "795Rq3e3lE_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"], output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "display(df_report)\n"
      ],
      "metadata": {
        "id": "vv8mdigtlI1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_true, y_pred, output_dict=True)\n",
        "print(\"ðŸ”¹ Macro Avg F1:\", report['macro avg']['f1-score'])\n",
        "print(\"ðŸ”¹ Weighted Avg F1:\", report['weighted avg']['f1-score'])\n"
      ],
      "metadata": {
        "id": "UHogZe89liez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarize labels for multi-class\n",
        "y_bin = label_binarize(y_true, classes=[0,1,2])\n",
        "probs = predictions_output.predictions\n",
        "print(\"ðŸ”¹ ROC AUC (macro):\", roc_auc_score(y_bin, probs, multi_class='ovr', average='macro'))\n"
      ],
      "metadata": {
        "id": "l6nDnsiHllT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_true and y_pred are obtained from your predictions and ground truth\n",
        "# y_true should be the true labels, and y_pred should be the predicted labels\n",
        "# Make sure y_true and y_pred are the same length\n",
        "\n",
        "# For example, let's assume you have:\n",
        "# y_true = true labels of your dataset\n",
        "# y_pred = predicted labels by the model\n",
        "# You can use trainer.predict() to get y_pred and the corresponding y_true\n",
        "\n",
        "# Get the true labels and predicted labels for the evaluation dataset\n",
        "# Note: `predictions_output.label_ids` is the true labels\n",
        "#       `predictions_output.predictions` contains the predicted probabilities\n",
        "#       Use np.argmax(predictions_output.predictions, axis=1) to get predicted labels\n",
        "\n",
        "y_pred = np.argmax(predictions_output.predictions, axis=1)\n",
        "y_true = predictions_output.label_ids\n",
        "\n",
        "# Now calculate precision, recall, and F1 score for each class\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred)\n",
        "\n",
        "# Define class labels\n",
        "classes = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "# Plot Precision, Recall, and F1 score for each class\n",
        "plt.figure(figsize=(8, 5))\n",
        "x = range(len(classes))\n",
        "\n",
        "plt.bar(x, prec, width=0.25, label='Precision')\n",
        "plt.bar([p + 0.25 for p in x], rec, width=0.25, label='Recall')\n",
        "plt.bar([p + 0.50 for p in x], f1, width=0.25, label='F1-Score')\n",
        "\n",
        "plt.xticks([p + 0.25 for p in x], classes)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"ðŸ“ˆ Precision, Recall, F1 per Class\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uyKrpVVYlpJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer for later use (e.g., Streamlit)\n",
        "model.save_pretrained(\"saved_model\")\n",
        "tokenizer.save_pretrained(\"saved_model\")\n",
        "\n",
        "# Zip them for easier upload\n",
        "import shutil\n",
        "shutil.make_archive(\"bert_sentiment_model\", 'zip', \"saved_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y3IZ_v3lorJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}